{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "#_set_env(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Plan-and-execute\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyC6FhO2BCsCoLp9aYjMnq59mzZ-hkKImi0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ULM stands for **Universal Language Model**. It\\'s a type of **deep learning model** that excels at natural language processing (NLP) tasks. \\n\\nHere\\'s a breakdown:\\n\\n**What makes it \"Universal\"?**\\n\\n* **Pre-trained on massive text datasets:**  ULMs are trained on vast amounts of text data, like Wikipedia, books, and online articles. This allows them to learn a broad understanding of language and its nuances.\\n* **Adaptable to various tasks:**  Once trained, ULMs can be fine-tuned for specific NLP tasks like:\\n    * **Text classification:**  Categorizing text (e.g., sentiment analysis, topic identification).\\n    * **Machine translation:**  Translating text from one language to another.\\n    * **Question answering:**  Answering questions based on provided text.\\n    * **Text summarization:**  Generating concise summaries of long texts.\\n    * **Named entity recognition:**  Identifying entities like people, organizations, and locations in text.\\n\\n**Key Features of ULM Models:**\\n\\n* **Bidirectional Language Model (BiLM):**  ULMs use BiLMs, which process text from both directions (left to right and right to left) to capture context more effectively.\\n* **Transfer Learning:**  ULMs leverage transfer learning, where knowledge learned during pre-training is transferred to specific NLP tasks. This saves time and resources compared to training from scratch.\\n* **Fine-tuning:**  ULMs are fine-tuned on specific datasets for the target task, further enhancing their performance.\\n\\n**Examples of ULM Models:**\\n\\n* **ELMo (Embeddings from Language Models):**  One of the earliest and influential ULMs.\\n* **GPT (Generative Pre-trained Transformer):**  A powerful ULM known for its text generation capabilities.\\n* **BERT (Bidirectional Encoder Representations from Transformers):**  A popular ULM that has achieved state-of-the-art results on many NLP tasks.\\n\\n**Benefits of Using ULM Models:**\\n\\n* **Improved accuracy:**  ULMs often outperform traditional NLP methods due to their deep learning capabilities and vast knowledge base.\\n* **Reduced training time:**  Pre-training allows for faster fine-tuning on specific tasks.\\n* **Versatility:**  ULMs can be applied to a wide range of NLP applications.\\n\\n**In summary, ULM models are powerful tools for NLP tasks. They leverage pre-training on massive text datasets and transfer learning to achieve high accuracy and efficiency.** \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is a ULM model\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'title' is not supported in schema, ignoring\n",
      "Key 'title' is not supported in schema, ignoring\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tools = [TavilySearchResults(max_results=3)]\n",
    "\n",
    "\n",
    "prompt = (\n",
    "    f\"You are a human study buddy. The user will provide you with sets of commands, either to solve questions for them or to generate questions. If provided with questions, answer comprehensively and simplify explanations as much as possible, assuming no prior knowledge from the user. If asked to generate questions, cover all relevant topics in the provided texts by giving the user as many questions as possible. If you don't have an answer to a particular question, use the available tools ({tools}) to search online for the latest information. Properly format your responses with backticks or LaTeX when necessary. Give multiple and varied answers, making your responses as comprehensive as possible. If the user's prompt is ambiguous or unclear, request clarification or provide a range of possible interpretations. Incorporate user feedback to improve your performance over time.\"\n",
    ")\n",
    "\n",
    "# We can add \"chat memory\" to the graph with LangGraph's checkpointer\n",
    "# to retain the chat context between interactions\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Define the graph\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "graph = create_react_agent(llm, tools=tools, checkpointer=memory,state_modifier=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Last premier league match\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I do not have access to real-time information, including sports schedules. To find the information about the last Premier League match, you can check websites like ESPN, BBC Sport, or the official Premier League website.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "inputs = {\"messages\": [(\"user\", \"Last premier league match\")]}\n",
    "\n",
    "print_stream(graph.stream(inputs, config=config, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_response(stream):\n",
    "    response = [x[\"messages\"] for x in stream]\n",
    "    return response[-1][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stream_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoast me based on my previous prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m)]}\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstream_response\u001b[49m(graph\u001b[38;5;241m.\u001b[39mstream(inputs, config\u001b[38;5;241m=\u001b[39mconfig, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stream_response' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"Roast me based on my previous prompts\")]}\n",
    "print(stream_response(graph.stream(inputs, config=config, stream_mode=\"values\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not have access to real-time information, including sports schedules. To find the information about the last Premier League match, you can check websites like ESPN, BBC Sport, or the official Premier League website. \\n\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [x[\"messages\"] for x in graph.stream(inputs, config=config, stream_mode=\"values\")]\n",
    "c[-1][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's Nyc known for?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I can't answer that. I only have information about the weather.  To learn more about NYC, you could try searching online!\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"What's Nyc known for?\")]}\n",
    "print_stream(graph.stream(inputs, config=config, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
